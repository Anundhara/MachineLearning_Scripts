# XGBoost Basics
1. XGBoost algorithm is better than random forests and neural networks in terms of accuracy, efficiency and feasibility 
2. Users need not spend more time feature engineering as this algorithm helps to save time and effort
3. XGBoost stands for eXtreme Gradient Boosting 
4. It can do parallel computing on a single machine
5. This algorithm works only with numeric vectors
6. If the dataset consists of categorical variables, then the user can convert such attributes into numerical data by one hot coding method
7. This is an example of xgboost model using the iris data available in base R.
8. Predict the Species from the 4 features of iris data.
9. The data contains numeric predictors. Our target column is Species, with 3 classes.
